import streamlit as st
import pandas as pd
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse
import base64

st.set_page_config(page_title="HyTech Web Scraper", layout="wide")
st.title("📊 HyTech Web Scraper & Comparison Tool")

def scrape_rating(url):
    try:
        res = requests.get(url, timeout=10)
        soup = BeautifulSoup(res.text, 'html.parser')
        for tag in soup.find_all(['span', 'div']):
            text = tag.get_text(strip=True)
            if "rating" in text.lower() and any(c.isdigit() for c in text):
                return text
        return "Not found"
    except:
        return "Error"

def count_images(url):
    try:
        res = requests.get(url, timeout=10)
        soup = BeautifulSoup(res.text, 'html.parser')
        return len(soup.find_all('img'))
    except:
        return "Error"

def find_hero_photo(url):
    try:
        res = requests.get(url, timeout=10)
        soup = BeautifulSoup(res.text, 'html.parser')
        imgs = soup.find_all('img')
        if not imgs:
            return "No images found"
        # Return the first image with width >= 600 if available
        for img in imgs:
            width = img.get('width')
            if width and int(width) >= 600 and img.get('src'):
                return img['src']
        return imgs[0]['src'] if imgs[0].get('src') else "Not found"
    except:
        return "Error"

uploaded_file = st.file_uploader("📤 Upload your CSV file", type=["csv"])
if uploaded_file:
    df_input = pd.read_csv(uploaded_file, encoding="ISO-8859-1")
    st.subheader("📥 Uploaded Data")
    st.dataframe(df_input)

    scrape_options = {
        "Current Rating": scrape_rating,
        "No. of Images": count_images,
        "Hero Photo?": find_hero_photo
    }

    st.subheader("🔍 Scraping Results")
    results = []

    for index, row in df_input.iterrows():
        result_row = {}
        for col in df_input.columns:
            if col in scrape_options:
                url = row[col]
                if isinstance(url, str) and url.startswith("http"):
                    result = scrape_options[col](url)
                else:
                    result = "Invalid URL"
                result_row[col] = result
        result_row["Website"] = urlparse(row[0]).netloc if isinstance(row[0], str) else f"Row {index+1}"
        results.append(result_row)

    df_results = pd.DataFrame(results)
    df_results = df_results[["Website"] + [col for col in df_results.columns if col != "Website"]]
    st.dataframe(df_results)

    csv = df_results.to_csv(index=False)
    b64 = base64.b64encode(csv.encode()).decode()
    href = f'<a href="data:file/csv;base64,{b64}" download="comparison_results.csv">📥 Download Results as CSV</a>'
    st.markdown(href, unsafe_allow_html=True)
